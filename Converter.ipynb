{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tool.config import Cfg\n",
    "from tool.translate import build_model, process_input, translate\n",
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "config = Cfg.load_config_from_file('ocr_model_cds_seq2seq/custom_config_seq2seq_12112025.yml')\n",
    "config['cnn']['pretrained']=False\n",
    "config['device'] = 'cpu'\n",
    "model, vocab = build_model(config)\n",
    "weight_path = 'ocr_model_cds_seq2seq/seq2seq.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "model.load_state_dict(torch.load(weight_path, map_location=torch.device(config['device'])))\n",
    "model = model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cnn_part(img, save_path, model, max_seq_length=128, sos_token=1, eos_token=2): \n",
    "    with torch.no_grad(): \n",
    "        src = model.cnn(img)\n",
    "        torch.onnx.export(model.cnn, img, save_path, export_params=True, opset_version=12, do_constant_folding=True, verbose=True, input_names=['img'], output_names=['output'], dynamic_axes={'img': {3: 'lenght'}, 'output': {0: 'channel'}})\n",
    "    \n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1249714/3405515023.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model.cnn, img, save_path, export_params=True, opset_version=12, do_constant_folding=True, verbose=True, input_names=['img'], output_names=['output'], dynamic_axes={'img': {3: 'lenght'}, 'output': {0: 'channel'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%img : Float(1, 3, 32, *, strides=[45600, 15200, 475, 1], requires_grad=0, device=cpu),\n",
      "      %model.last_conv_1x1.weight : Float(256, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu),\n",
      "      %model.last_conv_1x1.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_180 : Float(64, 3, 3, 3, strides=[27, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_181 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_183 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_184 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_186 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_187 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_189 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_190 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_192 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_193 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_195 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_196 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_198 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_199 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_201 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_202 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_204 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_205 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_207 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_208 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_210 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_211 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_213 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_214 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_216 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_217 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_219 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_220 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_222 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_223 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_225 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_226 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/model/features/features.0/Conv_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.0/Conv\"](%img, %onnx::Conv_180, %onnx::Conv_181), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.0 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.2/Relu_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.2/Relu\"](%/model/features/features.0/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.2 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.3/Conv_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.3/Conv\"](%/model/features/features.2/Relu_output_0, %onnx::Conv_183, %onnx::Conv_184), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.3 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.5/Relu_output_0 : Float(1, 64, 32, *, strides=[972800, 15200, 475, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.5/Relu\"](%/model/features/features.3/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.5 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.6/AveragePool_output_0 : Float(1, 64, 16, *, strides=[242688, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/model/features/features.6/AveragePool\"](%/model/features/features.5/Relu_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.6 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/pooling.py:773:0\n",
      "  %/model/features/features.7/Conv_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.7/Conv\"](%/model/features/features.6/AveragePool_output_0, %onnx::Conv_186, %onnx::Conv_187), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.7 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.9/Relu_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.9/Relu\"](%/model/features/features.7/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.9 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.10/Conv_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.10/Conv\"](%/model/features/features.9/Relu_output_0, %onnx::Conv_189, %onnx::Conv_190), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.10 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.12/Relu_output_0 : Float(1, 128, 16, *, strides=[485376, 3792, 237, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.12/Relu\"](%/model/features/features.10/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.12 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.13/AveragePool_output_0 : Float(1, 128, 8, *, strides=[120832, 944, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/model/features/features.13/AveragePool\"](%/model/features/features.12/Relu_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.13 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/pooling.py:773:0\n",
      "  %/model/features/features.14/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.14/Conv\"](%/model/features/features.13/AveragePool_output_0, %onnx::Conv_192, %onnx::Conv_193), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.14 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.16/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.16/Relu\"](%/model/features/features.14/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.16 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.17/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.17/Conv\"](%/model/features/features.16/Relu_output_0, %onnx::Conv_195, %onnx::Conv_196), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.17 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.19/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.19/Relu\"](%/model/features/features.17/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.19 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.20/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.20/Conv\"](%/model/features/features.19/Relu_output_0, %onnx::Conv_198, %onnx::Conv_199), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.20 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.22/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.22/Relu\"](%/model/features/features.20/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.22 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.23/Conv_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.23/Conv\"](%/model/features/features.22/Relu_output_0, %onnx::Conv_201, %onnx::Conv_202), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.23 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.25/Relu_output_0 : Float(1, 256, 8, *, strides=[241664, 944, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.25/Relu\"](%/model/features/features.23/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.25 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.26/AveragePool_output_0 : Float(1, 256, 4, *, strides=[120832, 472, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1], onnx_name=\"/model/features/features.26/AveragePool\"](%/model/features/features.25/Relu_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.26 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/pooling.py:773:0\n",
      "  %/model/features/features.27/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.27/Conv\"](%/model/features/features.26/AveragePool_output_0, %onnx::Conv_204, %onnx::Conv_205), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.27 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.29/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.29/Relu\"](%/model/features/features.27/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.29 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.30/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.30/Conv\"](%/model/features/features.29/Relu_output_0, %onnx::Conv_207, %onnx::Conv_208), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.30 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.32/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.32/Relu\"](%/model/features/features.30/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.32 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.33/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.33/Conv\"](%/model/features/features.32/Relu_output_0, %onnx::Conv_210, %onnx::Conv_211), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.33 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.35/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.35/Relu\"](%/model/features/features.33/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.35 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.36/Conv_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.36/Conv\"](%/model/features/features.35/Relu_output_0, %onnx::Conv_213, %onnx::Conv_214), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.36 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.38/Relu_output_0 : Float(1, 512, 4, *, strides=[241664, 472, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.38/Relu\"](%/model/features/features.36/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.38 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.39/AveragePool_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[2, 1], pads=[0, 0, 0, 0], strides=[2, 1], onnx_name=\"/model/features/features.39/AveragePool\"](%/model/features/features.38/Relu_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.39 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/pooling.py:773:0\n",
      "  %/model/features/features.40/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.40/Conv\"](%/model/features/features.39/AveragePool_output_0, %onnx::Conv_216, %onnx::Conv_217), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.40 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.42/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.42/Relu\"](%/model/features/features.40/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.42 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.43/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.43/Conv\"](%/model/features/features.42/Relu_output_0, %onnx::Conv_219, %onnx::Conv_220), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.43 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.45/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.45/Relu\"](%/model/features/features.43/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.45 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.46/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.46/Conv\"](%/model/features/features.45/Relu_output_0, %onnx::Conv_222, %onnx::Conv_223), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.46 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.48/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.48/Relu\"](%/model/features/features.46/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.48 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.49/Conv_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/features/features.49/Conv\"](%/model/features/features.48/Relu_output_0, %onnx::Conv_225, %onnx::Conv_226), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.conv.Conv2d::features.49 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/features/features.51/Relu_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Relu[onnx_name=\"/model/features/features.51/Relu\"](%/model/features/features.49/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.activation.ReLU::features.51 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/functional.py:1699:0\n",
      "  %/model/features/features.52/AveragePool_output_0 : Float(1, 512, 2, *, strides=[120832, 236, 118, 1], requires_grad=0, device=cpu) = onnx::AveragePool[ceil_mode=0, count_include_pad=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/features/features.52/AveragePool\"](%/model/features/features.51/Relu_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.container.Sequential::features/torch.nn.modules.pooling.AvgPool2d::features.52 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/pooling.py:773:0\n",
      "  %/model/last_conv_1x1/Conv_output_0 : Float(1, 256, 2, *, strides=[60416, 236, 118, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/last_conv_1x1/Conv\"](%/model/features/features.52/AveragePool_output_0, %model.last_conv_1x1.weight, %model.last_conv_1x1.bias), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model/torch.nn.modules.conv.Conv2d::last_conv_1x1 # /home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/nn/modules/conv.py:543:0\n",
      "  %/model/Transpose_output_0 : Float(1, 256, *, 2, strides=[60416, 236, 1, 118], requires_grad=0, device=cpu) = onnx::Transpose[perm=[0, 1, 3, 2], onnx_name=\"/model/Transpose\"](%/model/last_conv_1x1/Conv_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:40:0\n",
      "  %/model/Shape_output_0 : Long(4, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/model/Shape\"](%/model/Transpose_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Constant_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/model/Constant\"](), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Constant_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/model/Constant_1\"](), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Constant_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={2}, onnx_name=\"/model/Constant_2\"](), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Slice_output_0 : Long(2, strides=[1], device=cpu) = onnx::Slice[onnx_name=\"/model/Slice\"](%/model/Shape_output_0, %/model/Constant_1_output_0, %/model/Constant_2_output_0, %/model/Constant_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Constant_3_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/model/Constant_3\"](), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Concat_output_0 : Long(3, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/model/Concat\"](%/model/Slice_output_0, %/model/Constant_3_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %/model/Reshape_output_0 : Float(*, *, *, strides=[60416, 236, 1], requires_grad=0, device=cpu) = onnx::Reshape[onnx_name=\"/model/Reshape\"](%/model/Transpose_output_0, %/model/Concat_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:41:0\n",
      "  %output : Float(*, *, *, strides=[1, 60416, 236], requires_grad=0, device=cpu) = onnx::Transpose[perm=[2, 0, 1], onnx_name=\"/model/Transpose_1\"](%/model/Reshape_output_0), scope: model.backbone.cnn.CNN::/model.backbone.vgg.Vgg::model # /home/hungtrieu07/ConvertVietOcr2Onnx/model/backbone/vgg.py:42:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 32, 475)\n",
    "src = convert_cnn_part(img, './converted_weights/cnn.onnx', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder_part(model, src, save_path): \n",
    "    encoder_outputs, hidden = model.transformer.encoder(src) \n",
    "    torch.onnx.export(model.transformer.encoder, src, save_path, export_params=True, opset_version=11, do_constant_folding=True, input_names=['src'], output_names=['encoder_outputs', 'hidden'], dynamic_axes={'src':{0: \"channel_input\"}, 'encoder_outputs': {0: 'channel_output'}}) \n",
    "    return hidden, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1249714/2209043627.py:3: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model.transformer.encoder, src, save_path, export_params=True, opset_version=11, do_constant_folding=True, input_names=['src'], output_names=['encoder_outputs', 'hidden'], dynamic_axes={'src':{0: \"channel_input\"}, 'encoder_outputs': {0: 'channel_output'}})\n",
      "/home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hidden, encoder_outputs = convert_encoder_part(model, src, './converted_weights/encoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_decoder_part(model, tgt, hidden, encoder_outputs, save_path):\n",
    "    tgt = tgt[-1]\n",
    "    \n",
    "    torch.onnx.export(model.transformer.decoder,\n",
    "        (tgt, hidden, encoder_outputs),\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['tgt', 'hidden', 'encoder_outputs'],\n",
    "        output_names=['output', 'hidden_out', 'last'],\n",
    "        dynamic_axes={'encoder_outputs':{0: \"channel_input\"},\n",
    "                    'last': {0: 'channel_output'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = img.device\n",
    "tgt = torch.LongTensor([[1] * len(img)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1249714/3432815422.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model.transformer.decoder,\n",
      "/home/hungtrieu07/ConvertVietOcr2Onnx/model/seqmodel/seq2seq.py:93: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (output == hidden).all()\n",
      "/home/hungtrieu07/miniconda3/envs/vietocr_onnx/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "convert_decoder_part(model, tgt, hidden, encoder_outputs, './converted_weights/decoder.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = onnx.load('./converted_weights/cnn.onnx')\n",
    "decoder = onnx.load('./converted_weights/encoder.onnx')\n",
    "encoder = onnx.load('./converted_weights/decoder.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm model has valid schema\n",
    "onnx.checker.check_model(cnn)\n",
    "onnx.checker.check_model(decoder)\n",
    "onnx.checker.check_model(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1250044/3950294560.py:2: DeprecationWarning: Deprecated since 1.19. Consider using onnx.printer.to_text() instead.\n",
      "  onnx.helper.printable_graph(encoder.graph)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'graph main_graph (\\n  %tgt[INT64, 1]\\n  %hidden[FLOAT, 1x256]\\n  %encoder_outputs[FLOAT, channel_inputx1x512]\\n) initializers (\\n  %attention.attn.bias[FLOAT, 256]\\n  %embedding.weight[FLOAT, 233x256]\\n  %fc_out.weight[FLOAT, 233x1024]\\n  %fc_out.bias[FLOAT, 233]\\n  %onnx::MatMul_118[FLOAT, 768x256]\\n  %onnx::MatMul_119[FLOAT, 256x1]\\n  %onnx::GRU_137[FLOAT, 1x768x768]\\n  %onnx::GRU_138[FLOAT, 1x768x256]\\n  %onnx::GRU_139[FLOAT, 1x1536]\\n) {\\n  %/Unsqueeze_output_0 = Unsqueeze[axes = [0]](%tgt)\\n  %/embedding/Gather_output_0 = Gather(%embedding.weight, %/Unsqueeze_output_0)\\n  %/attention/Shape_output_0 = Shape(%encoder_outputs)\\n  %/attention/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/attention/Gather_output_0 = Gather[axis = 0](%/attention/Shape_output_0, %/attention/Constant_output_0)\\n  %/attention/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%hidden)\\n  %/attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/attention/Unsqueeze_1_output_0 = Unsqueeze[axes = [0]](%/attention/Gather_output_0)\\n  %/attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/attention/Concat_output_0 = Concat[axis = 0](%/attention/Constant_1_output_0, %/attention/Unsqueeze_1_output_0, %/attention/Constant_2_output_0)\\n  %/attention/Constant_3_output_0 = Constant[value = <Tensor>]()\\n  %/attention/Unsqueeze_2_output_0 = Unsqueeze[axes = [0]](%/attention/Gather_output_0)\\n  %/attention/Constant_4_output_0 = Constant[value = <Tensor>]()\\n  %/attention/Concat_1_output_0 = Concat[axis = 0](%/attention/Constant_3_output_0, %/attention/Unsqueeze_2_output_0, %/attention/Constant_4_output_0)\\n  %/attention/Shape_1_output_0 = Shape(%/attention/Concat_output_0)\\n  %/attention/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/attention/Shape_1_output_0)\\n  %/attention/Expand_output_0 = Expand(%/attention/Unsqueeze_output_0, %/attention/ConstantOfShape_output_0)\\n  %/attention/Tile_output_0 = Tile(%/attention/Expand_output_0, %/attention/Concat_1_output_0)\\n  %/attention/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%encoder_outputs)\\n  %/attention/Concat_2_output_0 = Concat[axis = 2](%/attention/Tile_output_0, %/attention/Transpose_output_0)\\n  %/attention/attn/MatMul_output_0 = MatMul(%/attention/Concat_2_output_0, %onnx::MatMul_118)\\n  %/attention/attn/Add_output_0 = Add(%attention.attn.bias, %/attention/attn/MatMul_output_0)\\n  %/attention/Tanh_output_0 = Tanh(%/attention/attn/Add_output_0)\\n  %/attention/v/MatMul_output_0 = MatMul(%/attention/Tanh_output_0, %onnx::MatMul_119)\\n  %/attention/Squeeze_output_0 = Squeeze[axes = [2]](%/attention/v/MatMul_output_0)\\n  %/attention/Softmax_output_0 = Softmax[axis = 1](%/attention/Squeeze_output_0)\\n  %/Unsqueeze_1_output_0 = Unsqueeze[axes = [1]](%/attention/Softmax_output_0)\\n  %/MatMul_output_0 = MatMul(%/Unsqueeze_1_output_0, %/attention/Transpose_output_0)\\n  %/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/MatMul_output_0)\\n  %/Concat_output_0 = Concat[axis = 2](%/embedding/Gather_output_0, %/Transpose_output_0)\\n  %/Unsqueeze_2_output_0 = Unsqueeze[axes = [0]](%hidden)\\n  %/rnn/GRU_output_0, %/rnn/GRU_output_1 = GRU[hidden_size = 256, linear_before_reset = 1](%/Concat_output_0, %onnx::GRU_137, %onnx::GRU_138, %onnx::GRU_139, %, %/Unsqueeze_2_output_0)\\n  %/rnn/Squeeze_output_0 = Squeeze[axes = [1]](%/rnn/GRU_output_0)\\n  %/Squeeze_output_0 = Squeeze[axes = [0]](%/embedding/Gather_output_0)\\n  %/Squeeze_1_output_0 = Squeeze[axes = [0]](%/rnn/Squeeze_output_0)\\n  %/Squeeze_2_output_0 = Squeeze[axes = [0]](%/Transpose_output_0)\\n  %/Concat_1_output_0 = Concat[axis = 1](%/Squeeze_1_output_0, %/Squeeze_2_output_0, %/Squeeze_output_0)\\n  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/Concat_1_output_0, %fc_out.weight, %fc_out.bias)\\n  %hidden_out = Squeeze[axes = [0]](%/rnn/GRU_output_1)\\n  %last = Squeeze[axes = [1]](%/Unsqueeze_1_output_0)\\n  return %output, %hidden_out, %last\\n}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(encoder.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./sample/35944.png')\n",
    "img = process_input(img, config['dataset']['image_height'], \n",
    "                config['dataset']['image_min_width'], config['dataset']['image_max_width'])  \n",
    "img = img.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mầm non: 141 thí sinh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = translate(img, model)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime's Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ONNX Runtime providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "Creating ONNX inference sessions...\n",
      "✓ ./converted_weights/cnn.onnx: Using CUDAExecutionProvider (GPU)\n",
      "✓ ./converted_weights/encoder.onnx: Using CUDAExecutionProvider (GPU)\n",
      "✓ ./converted_weights/decoder.onnx: Using CUDAExecutionProvider (GPU)\n",
      "\n",
      "All sessions created successfully!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "# Check available providers\n",
    "available_providers = onnxruntime.get_available_providers()\n",
    "print(f\"Available ONNX Runtime providers: {available_providers}\")\n",
    "\n",
    "# Function to create session with fallback to CPU if GPU fails\n",
    "def create_session_with_fallback(model_path, preferred_providers=None):\n",
    "    \"\"\"Create ONNX session, trying GPU first, falling back to CPU if GPU fails\"\"\"\n",
    "    if preferred_providers is None:\n",
    "        # Try GPU providers first\n",
    "        if 'CUDAExecutionProvider' in available_providers:\n",
    "            preferred_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        elif 'TensorrtExecutionProvider' in available_providers:\n",
    "            preferred_providers = ['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
    "        else:\n",
    "            preferred_providers = ['CPUExecutionProvider']\n",
    "    \n",
    "    try:\n",
    "        # Try to create session with preferred providers\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=preferred_providers)\n",
    "        # Check which provider was actually used\n",
    "        actual_provider = session.get_providers()[0]\n",
    "        if 'CUDA' in actual_provider or 'Tensorrt' in actual_provider:\n",
    "            print(f\"✓ {model_path}: Using {actual_provider} (GPU)\")\n",
    "        else:\n",
    "            print(f\"✓ {model_path}: Using {actual_provider} (CPU)\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        # If GPU fails, fall back to CPU\n",
    "        print(f\"⚠ GPU initialization failed for {model_path}, falling back to CPU\")\n",
    "        print(f\"  Error: {str(e)[:100]}...\")\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "        print(f\"✓ {model_path}: Using CPUExecutionProvider (CPU)\")\n",
    "        return session\n",
    "\n",
    "# Create inference sessions with automatic GPU/CPU fallback\n",
    "print(\"\\nCreating ONNX inference sessions...\")\n",
    "cnn_session = create_session_with_fallback(\"./converted_weights/cnn.onnx\")\n",
    "encoder_session = create_session_with_fallback(\"./converted_weights/encoder.onnx\")\n",
    "decoder_session = create_session_with_fallback(\"./converted_weights/decoder.onnx\")\n",
    "print(\"\\nAll sessions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_onnx(img, session, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    cnn_session, encoder_session, decoder_session = session\n",
    "\n",
    "    cnn_input = {cnn_session.get_inputs()[0].name: img}\n",
    "    src = cnn_session.run(None, cnn_input)[0]\n",
    "\n",
    "    encoder_input = {encoder_session.get_inputs()[0].name: src}\n",
    "    encoder_outputs, hidden = encoder_session.run(None, encoder_input)\n",
    "\n",
    "    translated_sentence = [[sos_token] * img.shape[0]]\n",
    "    max_length = 0\n",
    "\n",
    "    while max_length <= max_seq_length and not all(\n",
    "        np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n",
    "    ):\n",
    "        # Get the last token from the translated sentence (tgt should be the last token)\n",
    "        # translated_sentence[-1] is a list of tokens for all batch items at the last timestep\n",
    "        tgt_inp = np.asarray(translated_sentence[-1], dtype=np.int64)\n",
    "        # Ensure it's 1D: (batch_size,)\n",
    "        if len(tgt_inp.shape) > 1:\n",
    "            tgt_inp = tgt_inp.flatten()\n",
    "        \n",
    "        decoder_input = {\n",
    "            decoder_session.get_inputs()[0].name: tgt_inp,\n",
    "            decoder_session.get_inputs()[1].name: hidden,\n",
    "            decoder_session.get_inputs()[2].name: encoder_outputs,\n",
    "        }\n",
    "\n",
    "        logits, hidden, _ = decoder_session.run(None, decoder_input)\n",
    "        output = torch.from_numpy(logits)\n",
    "\n",
    "        values, indices = torch.topk(output, 1)\n",
    "        # Get the prediction for the current token\n",
    "        # indices shape is (batch_size, 1) after topk, so we take the first (and only) column\n",
    "        if len(indices.shape) == 2:\n",
    "            indices = indices[:, 0]\n",
    "        else:\n",
    "            indices = indices.squeeze()\n",
    "        indices = indices.tolist()\n",
    "\n",
    "        translated_sentence.append(indices)\n",
    "        max_length += 1\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.1838204250088893 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mầm non: 141 thí sinh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "session = (cnn_session, encoder_session, decoder_session)\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "img_np = img.detach().cpu().numpy()\n",
    "s = translate_onnx(img_np, session)[0].tolist()\n",
    "s = vocab.decode(s)\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vietocr_onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
